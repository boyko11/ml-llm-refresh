{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa978f86",
   "metadata": {},
   "source": [
    "# 04 - Neural Networks\n",
    "\n",
    "#### Let's get to the real deal ML - neural nets, aka Multilayer Perceptrons.\n",
    "\n",
    "We'll **build a minimal neural net from scratch** and use it to predict a single number from a synthetic dataset.  \n",
    "\n",
    "**Architecture**: 2 input vars, 1 layer of 4 hidden neurons, one continuous output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47a9fb",
   "metadata": {},
   "source": [
    "## What we'll do\n",
    "0. Setup & make a tiny synthetic dataset (2 features, 1 target)  \n",
    "1. Define the loss function (**MSE**)  \n",
    "2. Define the **forward pass** (with bias via input augmentation)  \n",
    "3**Backprop**: derive gradients and implement training loop (next, after you confirm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d763f9",
   "metadata": {},
   "source": [
    "## 0. Setup & synthetic data\n",
    "\n",
    "We'll create a small nonlinear target so the hidden layer is actually useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d8e122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:47:38.222953Z",
     "start_time": "2025-09-09T15:47:38.166683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x1=%{x}<br>x2=%{y}<br>y=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "q4LijYhe+D/rg496OIvzP60LcSP7efM/7KkmQUIZqj8zbyqsteUNwOYsCE0LAfA/tr12N8jy2j8Jt5naT0X2P+SVwMZk4AbAbQ35kVUNyr8oQtNEHOHCP9CMG6c4XfQ/biT+4q+gsj/H51yih2n3P8purVD29dc/XojEYjEUsL8uRm3DPQXjv6+3+Yj9fwjADSSkpJ3I+z+CpULMyrDWPzfoHT95peY/X7BFvn2647+So9KkNn7wP/ix8jIQyPc/1SNhz4O14z9Wl059L+HcP3BP0aJZIvq/AoiWjs/06b+f5FK3VoTjv6hVKwy3lLA/qitHfvqm4D8gNwK3ZcDzP0S5BgyvYu0/H4DIqBuvwL9e4XMkGjryP87rEJRSHLY//9IBevx19r+9eYEsKB3iP3TzCe/eFgnA43Cgk6ORrb/Lg+NIX7fpP3HoTOPJHfs/2GyCPbeuzT9g/ixtA0rlv4c+Y0BhBfM/uLCvssSm+j/MhS3COGvIv3zwzWJppva/KYRzlN3U5D9asm2/qGD5vxrsOT1D4+C/JlHZinLw5T8iow5NO9LuPzEq/XtaX9E/N8nPMPlh2D9pDRrfqoz3P2qjWY3+POk/Pbe1HRGM4L87b3ZHYn8JwBSSkhuxDu2/IAkoDqNGwz9KllKucQrpP0jS/g0M66m/xiB4x9ZP4T/CiLVFoSX4P+XfWm3H7uG/8P+Q2fSgsL/EfS3+dPj+v1whpD+qowjAomiUnPdU/L/DH5qFVbMGwBCMg3KFFfm/r/rpXBY45z8kMoR18MTlv1zk2n1hatC/TQJSN2qd9r+sFqbwp7H7v1QszS3C0+A/IGUy9Ue28T+TtVvGT8D4P6sYopgqccu/onolTFh79L/iOAiLyCntPzjIumiG6gLAB4KDLWmB578V7uhIiDH6v1pW0EyUZL8/wZeRmd9N4b/w5yWX4xTwvwVg/xeVRvK/zWX3Ylmf2j/G0j0q/xq5v0ojzM9RsvA/lE3ky1pD+r+EcPxifC3dP9iOXsuy3Pk/v97hbeWj8D97A8VxNInTPz33cq+CJLc/qGYzLX6G8z9+0MMglKj1PwSuvrqqbs0/Laas35og7b8OGf3njH/zP2vKScg2V/s/sjCuj2he7D9IWDVdrNTjv/7ydG+pANe/Q4eSHkKwAsAH4QhGDl3vPxWgNFUvgN2/N0vLAxv9979kyrB1FE0AwFZ7fMC1nri/Z93t42GV+b9+aSMc0m3xPxST3fhAr+g/EwnDkPqO8T96hf4Tyw30v/0oASz3Nfg/ctKZp28s+j9YUPLKjMjlP49Jr/lWlwDAZ3VBWBVFzr8Sdd/1/mPbP1AduhRq4Nc/yx4fZc3E1j97jHjzGsTwv9QBpJt5t9+/E0p9TPJprb/c9zjpG9mwv3RwgLSJpOw/ZaPrNK0i7j9VXuLfpvDnPxYKwoWMReG/anr915lv5L/MPsaad8vwP4SSaFpqguE/zPsTomUN5D9DCCCatV7rP+dUKlqbn+6/tFRgatZh9D9tw9G+36n2PxPfnAFbPOc/Njwd+HxA5T9cd2GJe065P9bf5xcpHOo/YHV9DtSx4L81X2Y2wonFv553yXWfCgnALcMN+zD18z8pz0i9PE/wPyiqW5v2KPK/u80azdE21z+qVm6dynH4v11q9IWFMuc/0wavOJCqvj+ZX9bc1+L0P5nJzBNpB+0/JtNc5WEJ8z8UXq6RQN/qP5FwyW+XO+k/fh1zM+hwBMDiaKaZlEEJwB6aZbmOZ+Y//n0pfSYV6r8rk3ctRWn4P3pUAIUU/fg/7Oclq9bA9r+zao7RLyX2P9yUk4Zcr6A/ZrRLcW8F5b865sDsWIb6v7Qy+Ng2yO4/ARBurgx8BcDbBj3pdxQHwF3TJ+mQ3fA/CvIHAhAJ8D87FqjqNezyP+digaeQL/c/TDbvTkW68z+CJb/e/oP4P8G8qdKy8+w/G7bYLQue67/ZcB1PJXfnP1iuJZL/N/s/ARViWMLwxz/DENtdsU3rP9oUmjeU29k/0pcGzkmb0D8dstMwDJX2P0TXAafLfQPADEq+ndoc6r88SZfQgc72PzanbuaXyAzAmkn72ZBu97/PGYX+rg7zP9DZmJxOTuw/76v2MEes878+KTcnYODqvw==",
           "dtype": "f8"
          },
          "coloraxis": "coloraxis",
          "opacity": 0.7,
          "size": 4,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "scene": "scene",
         "showlegend": false,
         "type": "scatter3d",
         "x": {
          "bdata": "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",
          "dtype": "f8"
         },
         "y": {
          "bdata": "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",
          "dtype": "f8"
         },
         "z": {
          "bdata": "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",
          "dtype": "f8"
         }
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "y"
          }
         },
         "colorscale": [
          [
           0.0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1.0,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "scene": {
         "domain": {
          "x": [
           0.0,
           1.0
          ],
          "y": [
           0.0,
           1.0
          ]
         },
         "xaxis": {
          "title": {
           "text": "x1"
          }
         },
         "yaxis": {
          "title": {
           "text": "x2"
          }
         },
         "zaxis": {
          "title": {
           "text": "y"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Interactive 3D view of the dataset"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Create synthetic data: N samples, 2 features\n",
    "N = 200\n",
    "x1 = rng.uniform(-2.0, 2.0, size=N)\n",
    "x2 = rng.uniform(-2.0, 2.0, size=N)\n",
    "X = np.stack([x1, x2], axis=1)  # shape (N, 2)\n",
    "\n",
    "# Nonlinear target with a bit of noise\n",
    "noise = rng.normal(0.0, 0.1, size=N)\n",
    "y = 1.2 * np.sin(x1) + 0.7 * x2 - 0.3 * x1 * x2 + noise\n",
    "y = y.reshape(y.size, 1)  # shape (N, 1)\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "# Now, let's plot it - this is a rotatable 3-D plot, drag it around to get familiar with the data\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Wrap into a DataFrame for nicer plotting\n",
    "df = pd.DataFrame({\"x1\": X[:,0], \"x2\": X[:,1], \"y\": y.squeeze()})\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df, x=\"x1\", y=\"x2\", z=\"y\",\n",
    "    color=\"y\", opacity=0.7,\n",
    "    title=\"Interactive 3D view of the dataset\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=4))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b7ff7",
   "metadata": {},
   "source": [
    "## 1. Let's Define the loss\n",
    "### Since we are predicting a single number, our old friend MSE make sense\n",
    "\n",
    "For a single example with prediction $ \\hat{y} $ and true value $ y $:\n",
    "$$\n",
    "L = \\tfrac{1}{2}(y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "For a batch of \\(N\\) examples:\n",
    "$$\n",
    "L = \\frac{1}{2N}\\sum_{i=1}^{N} \\left(y^{(i)} - \\hat{y}^{(i)}\\right)^2\n",
    "$$\n",
    "\n",
    "You probably noticed a difference with the previous MSE:  $ \\tfrac{1}{2} $ . This is a nice little trick tat makes the math cleaner - when we take a derivative of something that's squared, we end up with something that's 2x, so if we already have $ \\tfrac{1}{2} $, these two conveniently cancel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47228370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:47:38.251430Z",
     "start_time": "2025-09-09T15:47:38.243933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Test Success.\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Mean Squared Error with a 1/2 factor:\n",
    "    \"\"\"\n",
    "    return 0.5 * np.square(y_pred - y_true).mean()\n",
    "\n",
    "## lets test it real quick\n",
    "y_pred_fake = np.array([1, 2, 3, 4, 5])\n",
    "y_actual_fake = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "assert 0.0 == mse_loss(y_pred_fake, y_actual_fake)\n",
    "\n",
    "\n",
    "y_pred_fake = np.array([1, 2, 3, 4, 5])\n",
    "y_actual_fake = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# 1 + 4 + 9 + 16 + 25 = 55 / (2 * 5) = 5.5\n",
    "\n",
    "assert 5.5 == mse_loss(y_pred_fake, y_actual_fake)\n",
    "\n",
    "print(\"MSE Test Success.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899425d5948125",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1b1fe811eceed04",
   "metadata": {},
   "source": [
    "## 2. Forward pass (with biases via augmentation\n",
    "We’ll absorb biases by **augmenting** inputs with a constant 1.\n",
    "\n",
    "- Original input **X** has 2 features, so shape is `(N, 2)`.\n",
    "- $N$ in this case could be the number of datapoints we run through the network at the same time\n",
    "- During training N could be the number of datapoints of the entire datasets\n",
    "- More commonly N would be the number of data points in the training mini-batch \n",
    "- Augmented input **X** = `[X  1]` (append a column of ones), so shape is `(N, 3)`.  \n",
    "  From here on, **X** means the augmented one.\n",
    "\n",
    "### First layer (input to hidden, 4 neurons)\n",
    "\n",
    "- Weights `W[1]` has shape `(3, 4)`:\n",
    "  3 rows = augmented inputs, 4 columns = 4 neurons in the first hidden layer\n",
    "\n",
    "- Pre-activations:\n",
    "$$\n",
    "Z^{[1]} = X\\, W^{[1]}\n",
    "$$ \n",
    "- $Z$ is (N by 4), since $X$ is (N by 3) and we dotted it with $W^{[1]}$ which is (3 by 4)\n",
    "\n",
    "\n",
    "- Activations:\n",
    "$$\n",
    "H = \\sigma\\!\\left(Z^{[1]}\\right)\n",
    "$$\n",
    "\n",
    "### Hidden layer augmentation\n",
    "\n",
    "- Now since we are feeding the activations to another weight matrix, the one for the output layer, and we also want to have a bias unit for the output layer, we do the same augmentation trick for the activations output by our hidden layer\n",
    "$$\n",
    "H = [\\,H\\;\\;1\\,] \\quad \\in \\mathbb{R}^{N \\times 5}\n",
    "$$\n",
    "\n",
    "### Output layer (hidden to output, linear)\n",
    "\n",
    "- Weights `W[2]` has shape `(5, 1)`.\n",
    "\n",
    "- Predictions:\n",
    "$$\n",
    "\\hat{y} = H\\, W^{[2]} \\quad \\in \\mathbb{R}^{N \\times 1}\n",
    "$$\n",
    "\n",
    "### Sigmoid\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "#### Shape cheat-sheet\n",
    "    X: (N, 3)\n",
    "    \n",
    "    W[1]: (3, 4)\n",
    "    \n",
    "    Z[1], H: (N, 4)\n",
    "    \n",
    "    # Then we augment H\n",
    "    \n",
    "    H: (N, 5)\n",
    "    \n",
    "    W[2]: (5, 1)\n",
    "    \n",
    "    y_hat: (N, 1)\n",
    "    \n",
    "#### Neural Net Schematic\n",
    "![Neural net diagram](04_neural_net.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b86aab2e69396e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:47:38.338466Z",
     "start_time": "2025-09-09T15:47:38.335589Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb1ac8d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b5d95d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T15:47:38.388397Z",
     "start_time": "2025-09-09T15:47:38.378523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> X: (200, 2) | X_aug: (200, 3) | W1: (3, 4) | Z1: (200, 4) | H: (200, 4) | H_aug: (200, 5) | W2: (5, 1) | y_hat: (200, 1)\n",
      "Initial loss (untrained): 0.8242660887048511\n"
     ]
    }
   ],
   "source": [
    "def augment_with_ones(X: np.ndarray) -> np.ndarray:\n",
    "    '''Append a column of ones to X. If X is (N, d), returns (N, d+1).'''\n",
    "    ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "    return np.hstack([X, ones])\n",
    "\n",
    "X_test= np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "\n",
    "# print(\"Testing augment_with_ones\")\n",
    "# print(f\"X_test.shape: {X_test.shape}\")\n",
    "# X_test_augmented = augment_with_ones(X_test)\n",
    "# print(f\"X_test_augmented.shape: {X_test_augmented.shape}\")\n",
    "# print(\"========\")\n",
    "# print(\"X_test:\")\n",
    "# print(X_test)\n",
    "# print(\"========\")\n",
    "# print(\"X_test_augmented:\")\n",
    "# print(X_test_augmented)\n",
    "\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# print(\"========\")\n",
    "# H_sigmoid_test = np.array([-10, -5, -1, 0.1, 0.25, 0.5, 1, 5])\n",
    "# print(\"Sigmoid test for: \", H_sigmoid_test)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# print(sigmoid(H_sigmoid_test))\n",
    "\n",
    "def init_params(rng: np.random.Generator = np.random.default_rng(0), scale: float = 0.1):\n",
    "    '''Initialize weights for 2 to 4 to 1 MLP with bias via augmentation.'''\n",
    "    W1 = rng.normal(0.0, scale, size=(3, 4))  # (in+1=3) x (hidden=4)\n",
    "    W2 = rng.normal(0.0, scale, size=(5, 1))  # (hidden+1=5) x (out=1)\n",
    "    return {\"W1\": W1, \"W2\": W2}\n",
    "\n",
    "# print(\"========\")\n",
    "# print(\"Test init_params:\")\n",
    "# print(init_params())\n",
    "\n",
    "def forward(X: np.ndarray, params: dict):\n",
    "    '''\n",
    "    Forward pass.\n",
    "    X: shape (N, 2)\n",
    "    Returns: y_hat, cache\n",
    "    '''\n",
    "    W1, W2 = params[\"W1\"], params[\"W2\"]\n",
    "    X_aug = augment_with_ones(X)           # (N, 3)\n",
    "    Z1 = X_aug @ W1                        # (N, 4)\n",
    "    H = sigmoid(Z1)                        # (N, 4)\n",
    "    H_aug = augment_with_ones(H)           # (N, 5)\n",
    "    y_hat = H_aug @ W2                     # (N, 1) linear output\n",
    "    cache = {\"X_aug\": X_aug, \"Z1\": Z1, \"H\": H, \"H_aug\": H_aug, \"y_hat\": y_hat}\n",
    "    return y_hat, cache\n",
    "\n",
    "# Try a dry run\n",
    "params = init_params(rng)\n",
    "y_hat, cache = forward(X, params)\n",
    "print(\"Shapes -> X:\", X.shape, \"| X_aug:\", cache[\"X_aug\"].shape, \"| W1:\", params[\"W1\"].shape, \n",
    "      \"| Z1:\", cache[\"Z1\"].shape, \"| H:\", cache[\"H\"].shape, \"| H_aug:\", cache[\"H_aug\"].shape, \"| W2:\", params[\"W2\"].shape, \"| y_hat:\", y_hat.shape)\n",
    "print(\"Initial loss (untrained):\", mse_loss(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d6244918ffdefd",
   "metadata": {},
   "source": [
    "## 3. Backpropagation (baby steps)\n",
    "\n",
    "So far we know how to **predict** with our network:\n",
    "\n",
    "1. Take the inputs $X$ (augmented with a bias column).\n",
    "2. Multiply by the first weight matrix $W^{[1]}$.\n",
    "3. Apply the activation function $\\sigma$ to get hidden activations $H$.\n",
    "4. Augment $H$ with a bias column.\n",
    "5. Multiply by the second weight matrix $W^{[2]}$.\n",
    "6. Get the prediction $\\hat{y}$.\n",
    "\n",
    "In short, our forward pipeline looks like this:\n",
    "\n",
    "$$\n",
    "X \\;\\;\\xrightarrow{\\; W^{[1]} \\;}\\;\\; Z^{[1]} \\;\\;\\xrightarrow{\\;\\sigma\\;}\\;\\; H\n",
    "\\;\\;\\xrightarrow{\\;\\text{augment}\\;}\\;\\; H_{\\text{aug}}\n",
    "\\;\\;\\xrightarrow{\\; W^{[2]} \\;}\\;\\; \\hat{y}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Why backprop?\n",
    "\n",
    "We also have a **loss function** (MSE):\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2N} \\sum_{i=1}^N (y^{(i)} - \\hat{y}^{(i)})^2\n",
    "$$\n",
    "\n",
    "And your intuition is probably already telling you:  \n",
    "- If we can compute the **gradient** of this loss with respect to the weights,  \n",
    "- Then we can **update the weights** in the right direction to reduce the loss next time.\n",
    "\n",
    "That means:\n",
    "- First we’ll need to find the derivative of the loss function, $L$ with respect to $W^{[2]}$ (the output layer weights):  $\\frac{\\partial L}{\\partial W^{[2]}}$.  \n",
    "- Then we’ll also need to find the derivative of the loss function, $L$ with respect to $W^{[1]}$ (the hidden layer weights):  $\\frac{\\partial L}{\\partial W^{[1]}}$, since they influence $\\hat{y}$ indirectly.\n",
    "\n",
    "This is what backpropagation gives us:  \n",
    "a systematic way to push the error signal backwards through the network, layer by layer, using the chain rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6ce6146563f54",
   "metadata": {},
   "source": [
    "## 3a) Backprop: $\\frac{\\partial L}{\\partial W^{[2]}}$\n",
    "\n",
    "**Recall the forward bits we need:**\n",
    "- $H_{\\text{aug}} \\in \\mathbb{R}^{N\\times 5}$ (4 hidden activations + a column of 1s for bias)\n",
    "- $W^{[2]} \\in \\mathbb{R}^{5\\times 1}$\n",
    "- $\\hat{y} = H_{\\text{aug}}\\,W^{[2]} \\in \\mathbb{R}^{N\\times 1}$\n",
    "\n",
    "**Loss (MSE with $\\tfrac12$):**\n",
    "$$\n",
    "L = \\frac{1}{2N}\\sum_{i=1}^N \\bigl(\\hat{y}^{(i)} - y^{(i)}\\bigr)^2\n",
    "$$\n",
    "\n",
    "**As in earlier notebooks, we can derive the gradient for a single datapoint and then average. So ignore the sum for a moment and use:**\n",
    "$$\n",
    "L = \\tfrac{1}{2}\\,(\\hat{y} - y)^2\n",
    "$$\n",
    "\n",
    "### Step 1: gradient w.r.t. prediction (single datapoint)\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} \\;=\\; \\hat{y} - y\n",
    "$$\n",
    "\n",
    "### Step 2: chain rule to get gradient w.r.t. $W^{[2]}$ (single datapoint)\n",
    "Since $\\hat{y} = H_{\\text{aug}}\\,W^{[2]}$ and (for one example) $H_{\\text{aug}}$ is a row $(1\\times 5)$,\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}}{\\partial W^{[2]}} \\;=\\; H_{\\text{aug}}^\\top \\quad (5\\times 1)\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[2]}}\n",
    "\\;=\\;\n",
    "\\frac{\\partial L}{\\partial \\hat{y}}\n",
    "\\cdot\n",
    "\\frac{\\partial \\hat{y}}{\\partial W^{[2]}}\n",
    "\\;=\\;\n",
    "(\\hat{y} - y)\\,H_{\\text{aug}}^\\top\n",
    "\\quad\\in\\mathbb{R}^{5\\times 1}.\n",
    "$$\n",
    "\n",
    "### Turn it into the batch average\n",
    "Over $N$ examples:\n",
    "$$\n",
    "\\boxed{\\;\n",
    "\\frac{\\partial L}{\\partial W^{[2]}}\n",
    "\\;=\\;\n",
    "\\frac{1}{N}\\,H_{\\text{aug}}^{\\!\\top}\\,(\\hat{y}-y)\n",
    "\\;}\n",
    "\\quad\\text{with}\\quad\n",
    "H_{\\text{aug}}\\in\\mathbb{R}^{N\\times 5},\\;\n",
    "(\\hat{y}-y)\\in\\mathbb{R}^{N\\times 1}.\n",
    "$$\n",
    "\n",
    "**Shape check:** $(5\\times N)\\cdot(N\\times 1)=(5\\times 1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2163ee3a3a9782",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:52:53.927124Z",
     "start_time": "2025-09-09T17:52:53.923378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "# y_hat, H_aug from the forward pass (e.g., via cache)\n",
    "y_hat = cache[\"y_hat\"]     # (N, 1)\n",
    "H_aug = cache[\"H_aug\"]     # (N, 5)\n",
    "\n",
    "N = y.shape[0]\n",
    "dL_dy_hat = (y_hat - y) / N      # (N, 1)\n",
    "grad_W2   = H_aug.T @ dL_dy_hat  # (5, 1)\n",
    "\n",
    "print(dL_dy_hat.shape)  # (N, 1)\n",
    "print(grad_W2.shape)    # (5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31e461f1a6b57e",
   "metadata": {},
   "source": [
    "#### Well, kewl, now we have the loss gradient with respect with W_2\n",
    "That's enough information to allows to update the weights in $W^{[2]}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04abb923233763",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "151cd2b65dc4004f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T17:56:10.201151Z",
     "start_time": "2025-09-09T17:56:10.197458Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "w2_before_update = params[\"W2\"]\n",
    "assert np.array_equal(params[\"W2\"], w2_before_update)\n",
    "params[\"W2\"] = params[\"W2\"] - learning_rate * grad_W2\n",
    "assert not np.array_equal(params[\"W2\"], w2_before_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60717dbe50213eee",
   "metadata": {},
   "source": [
    "### Baby Step 3 — Translate hidden error into updates for $W^{[1]}$\n",
    "\n",
    "Now that each hidden neuron has its **error signal** (from Baby Step 2),  \n",
    "we can finally connect that signal back to the weights $W^{[1]}$ that created those hidden activations.\n",
    "\n",
    "Here’s the intuition:\n",
    "\n",
    "- Each hidden neuron $h_j$ is the result of a **weighted sum of inputs** (plus bias):  \n",
    "  $$\n",
    "  z^{[1]}_j = x_1 \\cdot W^{[1]}_{1,j} \\;+\\; x_2 \\cdot W^{[1]}_{2,j} \\;+\\; 1 \\cdot W^{[1]}_{\\text{bias},j}\n",
    "  $$\n",
    "\n",
    "- If a hidden neuron ended up contributing to the output error,  \n",
    "  then all of the weights that fed into it should be nudged a little.\n",
    "\n",
    "- How much should each weight change? Two factors matter:\n",
    "  1. **The input value** that came through that connection (if an input was 0, that weight didn’t matter this time).  \n",
    "  2. **The hidden error** for that neuron (if the neuron had no error signal, its weights don’t need to move).\n",
    "\n",
    "Put together:\n",
    "\n",
    "> **The gradient for $W^{[1]}$ is basically the input values multiplied by the hidden errors.**\n",
    "\n",
    "In matrix form, this becomes an outer product:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[1]}} = X_{\\text{aug}}^\\top \\cdot \\text{hidden\\_error}\n",
    "$$\n",
    "\n",
    "- $X_{\\text{aug}}$: input with the bias column, shape $(N \\times 3)$  \n",
    "- $\\text{hidden\\_error}$: error signals for the 4 hidden neurons, shape $(N \\times 4)$  \n",
    "- Result: gradient for $W^{[1]}$, shape $(3 \\times 4)$\n",
    "\n",
    "For completeness, we can substitute the hidden\\_error term from Baby Step 2:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{[1]}}\n",
    "= X_{\\text{aug}}^\\top \\cdot \\Bigl( \\bigl((\\hat{y} - y) \\times W^{[2]}_{\\text{(no bias)}}\\bigr) \\;\\times\\; (H \\cdot (1 - H)) \\Bigr)\n",
    "$$\n",
    "\n",
    "This gradient tells us exactly how to nudge each input-to-hidden weight so that next time,  \n",
    "the hidden activations $H$ will be a little closer to what we wished they were.\n",
    "\n",
    "---\n",
    "\n",
    "### Minimal NumPy snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10b5566942ada17a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T20:38:57.973996Z",
     "start_time": "2025-09-09T20:38:57.968039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# From the forward pass / cache:\n",
    "X_aug = cache[\"X_aug\"]      # (N, 3)\n",
    "H      = cache[\"H\"]         # (N, 4)\n",
    "y_hat  = cache[\"y_hat\"]     # (N, 1)\n",
    "W2     = params[\"W2\"]       # (5, 1)\n",
    "\n",
    "N = y.shape[0]\n",
    "\n",
    "# Hidden blame (drop the bias weight from W2)\n",
    "hidden_blame = (y_hat - y) @ W2[:-1].T      # (N, 4)\n",
    "\n",
    "# Sigmoid sensitivity\n",
    "sigmoid_sensitivity = H * (1 - H)           # (N, 4)\n",
    "\n",
    "# Hidden error\n",
    "hidden_error = hidden_blame * sigmoid_sensitivity   # (N, 4)\n",
    "\n",
    "# Gradient for W1\n",
    "grad_W1 = X_aug.T @ hidden_error            # (3, 4)\n",
    "\n",
    "print(grad_W1.shape)  # (3, 4)\n",
    "\n",
    "\n",
    "## And now we could \"update\" W1 if we wanted to\n",
    "w1_before_update = params[\"W1\"]\n",
    "assert np.array_equal(params[\"W1\"], w1_before_update)\n",
    "params[\"W1\"] = params[\"W1\"] - learning_rate * grad_W1\n",
    "assert not np.array_equal(params[\"W1\"], w1_before_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716d4fd",
   "metadata": {},
   "source": [
    "\n",
    "# Formal backprop derivation (optional, specific to our network)\n",
    "\n",
    "---\n",
    "\n",
    "# Step 1 — Output layer (matrix style, but still using fractions)\n",
    "\n",
    "**Prediction:**\n",
    "\n",
    "$$\n",
    "\\hat y = W_2 \\, h\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $W_2$ is a $1 \\times 5$ row vector of weights\n",
    "* $h$ is a $5 \\times 1$ column vector of hidden activations (with the bias “1” included)\n",
    "\n",
    "So $\\hat y$ is just a scalar.\n",
    "\n",
    "---\n",
    "\n",
    "**Loss:**\n",
    "\n",
    "$$\n",
    "L = \\tfrac{1}{2}(\\hat y - y)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1A: derivative of the loss w\\.r.t. the prediction\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat y} = \\hat y - y\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1B: derivative of the prediction w\\.r.t. the weight vector\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "\\hat y = W_2 h,\n",
    "$$\n",
    "\n",
    "we can view $\\hat y$ as a dot product of the row vector $W_2$ with the column vector $h$.\n",
    "\n",
    "The derivative of a dot product w\\.r.t. the row vector is just the column vector transposed:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat y}{\\partial W_2} = h^\\top\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Why is it $h^\\top$ and not just $h$?\n",
    "\n",
    "* $W_2$ is a row vector of shape $1 \\times 5$.\n",
    "* The gradient must have the same shape.\n",
    "* Writing $h^\\top$ (a row vector) ensures the shapes line up: both are $1 \\times 5$.\n",
    "* If we wrote just $h$ (a column), the shapes wouldn’t match.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1C: chain rule\n",
    "\n",
    "Now combine everything:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2}\n",
    "= \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial W_2}\n",
    "= (\\hat y - y) \\, h^\\top\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Step 2 — Hidden layer\n",
    "\n",
    "**Hidden activations:**\n",
    "\n",
    "$$\n",
    "h = \\sigma(z_1), \\quad z_1 = W_1 x\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $W_1$ is a $4 \\times 3$ weight matrix (2 inputs + bias, 4 hidden units)\n",
    "* $x$ is a $3 \\times 1$ input vector (augmented with bias “1”)\n",
    "* $h$ is a $4 \\times 1$ column of sigmoid outputs, later augmented with a bias “1” before going to the output layer\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2A: derivative of the loss w\\.r.t. the hidden activations\n",
    "\n",
    "From Step 1 we had\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat y} = \\hat y - y\n",
    "$$\n",
    "\n",
    "And $\\hat y = W_2 h$.\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat y}{\\partial h} = W_2^\\top\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h} = \\frac{\\partial L}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial h}\n",
    "= (\\hat y - y) \\, W_2^\\top\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2B: derivative of hidden activations w\\.r.t. pre-activations\n",
    "\n",
    "Each hidden unit uses the sigmoid:\n",
    "\n",
    "$$\n",
    "h = \\sigma(z_1), \\quad \\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h}{\\partial z_1} = h \\odot (1 - h)\n",
    "$$\n",
    "\n",
    "(where $\\odot$ means elementwise product).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2C: chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_1} =\n",
    "\\left[ \\frac{\\partial L}{\\partial h} \\right] \\odot \\left[ \\frac{\\partial h}{\\partial z_1} \\right]\n",
    "= \\big( (\\hat y - y) \\, W_2^\\top \\big) \\odot \\big(h \\odot (1-h)\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "This gives us the “hidden error signal” (often written $\\delta_1$):\n",
    "\n",
    "$$\n",
    "\\delta_1 = \\big( (\\hat y - y) \\, W_2^\\top \\big) \\odot \\big(h \\odot (1-h)\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Next in **Step 3**, we’ll use this $\\delta_1$ to compute the gradient w\\.r.t. the first-layer weights $W_1$.\n",
    "\n"
   ],
   "id": "ea331e2c694005bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Awesome—straight to it.\n",
    "\n",
    "# Step 3 - First layer weights $W_1$\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "$$\n",
    "z_1 = W_1\\,x,\\qquad h=\\sigma(z_1),\\qquad \\delta_1=\\Big((\\hat y-y)\\,W_2^\\top\\Big)\\odot\\big(h\\odot(1-h)\\big)\n",
    "$$\n",
    "\n",
    "* $W_1 \\in \\mathbb{R}^{4\\times 3}$ (2 inputs + bias), $x\\in\\mathbb{R}^{3\\times 1}$.\n",
    "* $\\delta_1 \\in \\mathbb{R}^{4\\times 1}$ corresponds to the 4 hidden neurons (not the bias unit).\n",
    "\n",
    "---\n",
    "\n",
    "### 3A) Derivative of $z_1$ w\\.r.t. $W_1$\n",
    "\n",
    "Each hidden pre-activation is linear in $W_1$:\n",
    "\n",
    "$$\n",
    "z_1 = W_1 x \\;\\;\\Longrightarrow\\;\\; \\frac{\\partial z_1}{\\partial W_1} \\text{ acts like “multiply by } x^\\top\\text{”.}\n",
    "$$\n",
    "\n",
    "Concretely, for any entry $W_{1,ik}$, $\\frac{\\partial z_{1,i}}{\\partial W_{1,ik}} = x_k$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3B) Chain rule to get $\\frac{\\partial L}{\\partial W_1}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1}\n",
    "= \\frac{\\partial L}{\\partial z_1}\\;\\frac{\\partial z_1}{\\partial W_1}\n",
    "= \\delta_1 \\; x^\\top\n",
    "$$\n",
    "\n",
    "That’s an **outer product**: column $\\delta_1$ times row $x^\\top$.\n",
    "\n",
    "---\n",
    "\n",
    "### 3C) Shapes (quick check)\n",
    "\n",
    "* $\\delta_1$ is $4\\times 1$\n",
    "* $x^\\top$ is $1\\times 3$\n",
    "* $\\delta_1 x^\\top$ is $4\\times 3$  - matches $W_1$\n",
    "\n",
    "---\n",
    "\n",
    "### Mini-batch version (mean over $N$ examples)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\bar L}{\\partial W_1} \\;=\\; \\frac{1}{N}\\sum_{n=1}^N \\delta_1^{(n)} \\,{x^{(n)}}^\\top\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Tiny intuition\n",
    "\n",
    "* Each weight $W_{1,ik}$ connects input feature $x_k$ into hidden neuron $i$.\n",
    "* Its update is proportional to **(error at that hidden neuron)** $\\delta_{1,i}$ times **(that input feature)** $x_k$.\n",
    "\n",
    "---\n"
   ],
   "id": "5e1991f0bacecc49"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
