{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Important Takeaways So Far  \n",
    "\n",
    "Hey, pretty cool — we’ve now tackled our very first ML examples:  \n",
    "\n",
    "- **Linear Regression**:  \n",
    "  We found the best-fit line (or plane, if you imagine higher dimensions).  \n",
    "  In our 2D case, the data was described by two features, and we learned the parameters (weights) for a line:  \n",
    "\n",
    "  $$\n",
    "  w_1 \\cdot x_1 + w_2 \\cdot x_2 + w_0 = y\n",
    "  $$\n",
    "\n",
    "  - Here $w_1, w_2, w_0$ are the weights (parameters) the model learns.  \n",
    "  - Once we know these, we can plug in the feature values $(x_1, x_2)$ to predict $y$.  \n",
    "\n",
    "- **Logistic Regression**:  \n",
    "  Very similar structure, but this time our goal was **classification**.  \n",
    "  Instead of predicting a number directly, we learned weights that indirectly define a **decision boundary** — a line (in 2D) or plane (in higher dimensions) — that separates one class from another.  \n",
    "\n",
    "---\n",
    "\n",
    "## The Training Loop\n",
    "\n",
    "You probably noticed something interesting: we basically **copy–pasted the training loop logic** from linear regression into logistic regression!  \n",
    "The only things that changed were:  \n",
    "- The **loss function** (MSE → BCE),  \n",
    "- The **gradient calculation** (because it depends on the loss).  \n",
    "\n",
    "Everything else was the same.  \n",
    "\n",
    "Here’s the general recipe you’ve seen:  \n",
    "\n",
    "1. **Initialize parameters (weights)** randomly.  \n",
    "2. For a certain number of **epochs** (an epoch = one full pass over the training set):  \n",
    "   - For each training record (or a batch — more on that below):  \n",
    "     - Plug the features into the model → get a prediction $\\hat{y}$.  \n",
    "     - Compare $\\hat{y}$ with the true label → compute the **loss**.  \n",
    "     - Express the loss in terms of the weights.  \n",
    "     - Compute the **gradient**: this tells us how the loss changes with respect to each weight, and in which direction the loss is increasing.  \n",
    "     - Update the weights in the **opposite direction** of the gradient, since we want to move toward *lower* loss, not higher.\n",
    "\n",
    "---\n",
    "\n",
    "## Vectorization vs. Batching\n",
    "\n",
    "- In practice, we don’t literally write a `for` loop over each record.  \n",
    "  Instead, we **vectorize**: a couple of lines of NumPy will compute the average loss and gradient for the *entire dataset at once*.  \n",
    "\n",
    "- But for really large datasets, we don’t process everything in one go either.  \n",
    "  Instead, we use **mini-batches**:  \n",
    "  - Take small subsets (e.g. 16, 32, 64, 128 records).  \n",
    "  - Do the same forward → loss → gradient → update loop, just on that batch.  \n",
    "  - Repeat until we’ve seen all batches → that’s one epoch.  \n",
    "\n",
    "This batching is the secret sauce that makes modern ML training feasible.  \n",
    "\n",
    "---\n",
    "\n",
    "## Main Takeaway\n",
    "\n",
    "At its core, the **training loop is roughly the same across many ML algorithms**.  \n",
    "The only things that change are:  \n",
    "- The **loss function** (depends on the problem: regression, classification, etc.),  \n",
    "- The resulting **gradient** (because it comes from the loss).  \n",
    "\n",
    "Everything else — initialization, looping over data, updating weights — is the same rhythm.  \n"
   ],
   "id": "e6c156197fe33ba3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eab310d48f2be8d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
